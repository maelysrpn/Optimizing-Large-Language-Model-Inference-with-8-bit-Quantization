{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1e644cf761a476e8d0dedbbf9be43c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d480ed5f89ff49a689b94fedc379e805",
              "IPY_MODEL_667c77b001314a609d8a632f27f5e5eb",
              "IPY_MODEL_09cf8a1ae7934685b947a3946189ab05"
            ],
            "layout": "IPY_MODEL_398e959a79354da38489618ff07d4609"
          }
        },
        "d480ed5f89ff49a689b94fedc379e805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_563f4c6072b5419c86db717cbd596236",
            "placeholder": "​",
            "style": "IPY_MODEL_e9a8fe4749db462b9fc584073e09d524",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "667c77b001314a609d8a632f27f5e5eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b7f80160f114f84bfc145f6f770137e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2cf406ae63d42769f4b88ca97bd7ccc",
            "value": 2
          }
        },
        "09cf8a1ae7934685b947a3946189ab05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_817705a39e8e4e99b6133276eb62f5ae",
            "placeholder": "​",
            "style": "IPY_MODEL_6eeae9eb0e414d009abc1e4ca6172a8b",
            "value": " 2/2 [00:56&lt;00:00, 25.20s/it]"
          }
        },
        "398e959a79354da38489618ff07d4609": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "563f4c6072b5419c86db717cbd596236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9a8fe4749db462b9fc584073e09d524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b7f80160f114f84bfc145f6f770137e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2cf406ae63d42769f4b88ca97bd7ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "817705a39e8e4e99b6133276eb62f5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6eeae9eb0e414d009abc1e4ca6172a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6e55a612604340b2981526cd66536b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12aeb463a225488da1d6f81949f15c3f",
              "IPY_MODEL_6beccb3dceb146178fcea7c8910274e9",
              "IPY_MODEL_819c5b74ecb74e1d83b5e753c16550cd"
            ],
            "layout": "IPY_MODEL_286710677627418195dd976618c61d51"
          }
        },
        "12aeb463a225488da1d6f81949f15c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc7221f23a04da2bd7f7210411a6e96",
            "placeholder": "​",
            "style": "IPY_MODEL_c0d1364b6d114dafae3ba3c08eab3c8f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6beccb3dceb146178fcea7c8910274e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cad1737498174907b213bf0e6f013e7d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e7d845c950f4143b0ac5fef3bf4d885",
            "value": 2
          }
        },
        "819c5b74ecb74e1d83b5e753c16550cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_933b91dc2e5c4336be0b9047e2545338",
            "placeholder": "​",
            "style": "IPY_MODEL_65fc39dca8bb4deaafaea9b2200d0f72",
            "value": " 2/2 [01:10&lt;00:00, 31.67s/it]"
          }
        },
        "286710677627418195dd976618c61d51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc7221f23a04da2bd7f7210411a6e96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0d1364b6d114dafae3ba3c08eab3c8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cad1737498174907b213bf0e6f013e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e7d845c950f4143b0ac5fef3bf4d885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "933b91dc2e5c4336be0b9047e2545338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65fc39dca8bb4deaafaea9b2200d0f72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LIBRARY INSTALLATION"
      ],
      "metadata": {
        "id": "xOrZ7w6XwFS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rDfiBh05pN3",
        "outputId": "9e640f3f-a80b-49a5-8a3f-2bf8718f4dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m838.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed successfully.\n"
          ]
        }
      ],
      "source": [
        "# transformers: The main Hugging Face library. It provides the 'AutoModelForCausalLM'\n",
        "#    class to load the LLaMA architecture and the 'AutoTokenizer' to process text.\n",
        "\n",
        "# accelerate: A library by Hugging Face to handle hardware acceleration.\n",
        "#    It is useful here for the 'device_map=\"auto\"' feature, which efficiently manages\n",
        "#    how the large model is loaded onto the GPU memory.\n",
        "\n",
        "# bitsandbytes: The core library for quantization. It allows us to use the\n",
        "#    8-bit quantization described in the paper (using 'load_in_8bit=True').\n",
        "\n",
        "# scipy: Needed for complex mathematical operations for transformers\n",
        "\n",
        "# torch: Deep Learning framework\n",
        "\n",
        "!pip install -q -U transformers accelerate bitsandbytes scipy torch\n",
        "print(\"Dependencies installed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SET UP AND FUNCTIONS\n",
        "\n"
      ],
      "metadata": {
        "id": "MrFISTQ2v1AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import gc\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "eWLWCIDi92Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will compare the performance of the 8-bit model to the 16 and not 32-bit model. In fact, the free Colab GPU would crash when loading the 32 bits model. It only has 15 Giga bytes while the 32-bit requires 28 Giga bytes of VRAM."
      ],
      "metadata": {
        "id": "uZw4tRWrqQ2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Chat version of LLaMA 2 (7 billion parameters)\n",
        "\n",
        "# 1. Model Selection:\n",
        "# We use \"NousResearch/Llama-2-7b-chat-hf\".\n",
        "# - \"Llama-2-7b\": The 7 Billion parameter version is the standard size that fits\n",
        "#   on the Colab GPU.\n",
        "# - \"NousResearch\": We use this third-party mirror because the official Meta weights\n",
        "#   require a gated license approval process. This version is architecture-identical\n",
        "#   but publicly accessible immediately.\n",
        "# - \"chat\": This model is fine-tuned for instructions, ensuring it answers questions\n",
        "#   rather than just completing text.\n",
        "MODEL_ID = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# 2. Benchmark Prompt:\n",
        "# We select a complex prompt to ensure the model generates a long enough sequence.\n",
        "# This allows us to:\n",
        "# - Accurately measure generation speed (tokens/sec) over a sustained period.\n",
        "# - Qualitatively assess if quantization affects the logical coherence of the plan.\n",
        "PROMPT = \"Write a detailed plan to visit Paris in 3 days, focusing on museums and food.\"\n",
        "\n",
        "def flush_memory():\n",
        "    \"\"\"\n",
        "    Function to clear GPU VRAM between model loads.\n",
        "    Prevents running out of memory on Google Colab.\n",
        "    \"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(\"GPU Memory flushed.\\n\")\n",
        "\n",
        "def get_memory_usage():\n",
        "    \"\"\"\n",
        "    Returns the current GPU memory allocation in Gigabyte.\n",
        "    \"\"\"\n",
        "    return torch.cuda.memory_allocated() / (1024 ** 3)\n",
        "\n",
        "def run_benchmark(model, tokenizer, name):\n",
        "    \"\"\"\n",
        "    Performance and quality benchmark on the loaded model.\n",
        "    \"\"\"\n",
        "    print(f\"--- Benchmarking Model: {name} ---\")\n",
        "\n",
        "    # 1. Measure Memory Footprint\n",
        "    # We measure how much VRAM the model weights occupy on the GPU.\n",
        "    memory_footprint = get_memory_usage()\n",
        "    print(f\"VRAM Memory Footprint: {memory_footprint:.2f} GB\")\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # 2. Warmup Phase\n",
        "    # The first inference is always slower due to CUDA kernel initialization.\n",
        "    # We run a short generation to 'warm up' the GPU for accurate timing later.\n",
        "    print(\"Warming up GPU\")\n",
        "    _ = model.generate(**inputs, max_new_tokens=20)\n",
        "\n",
        "    # 3. Inference Speed & Quality Test\n",
        "    print(\"Generating text for measurement\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # We generate exactly 100 new tokens to ensure a fair comparison between models.\n",
        "    output = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate Metrics\n",
        "    latency = end_time - start_time\n",
        "    tokens_per_sec = 100 / latency\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Time: {latency:.2f}s | Speed: {tokens_per_sec:.2f} tokens/s\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Memory (GB)\": round(memory_footprint, 2),\n",
        "        \"Latency (s)\": round(latency, 2),\n",
        "        \"Tokens/Sec\": round(tokens_per_sec, 2),\n",
        "        \"Output Sample quick\": generated_text[:200] + \"...\" , # Keep a small sample to quickly check the quality\n",
        "        \"Output Sample\": generated_text # Full output for comparison\n",
        "    }"
      ],
      "metadata": {
        "id": "0fkmP0rT_exy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE MODEL BENCHMARK (FP16)"
      ],
      "metadata": {
        "id": "eEYMUOiYvCbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We start by evaluating the model in Half-Precision (FP16).\n",
        "print(\"Loading Baseline Model (FP16)\")\n",
        "\n",
        "# Loading the Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Loading the Model weights\n",
        "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    # We explicitly load in float16 to fit in memory while maintaining high precision.\n",
        "    dtype=torch.float16,\n",
        "    # 'accelerate' library automatically handles the placement of layers on the GPU.\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Executing the Benchmark\n",
        "res_fp16 = run_benchmark(model_fp16, tokenizer, \"LLaMA-2-7B (FP16)\")\n",
        "\n",
        "# Cleaning up phase\n",
        "# The Colab GPU cannot hold both the FP16 and the 8-bit model\n",
        "# in memory simultaneously. Thus, we must delete the first model and force-clear the\n",
        "# GPU cache to prevent an \"Out Of Memory\" (OOM) crash before the next step\n",
        "\n",
        "print(\"Cleaning up FP16 model to free VRAM\")\n",
        "del model_fp16\n",
        "del tokenizer\n",
        "flush_memory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "c1e644cf761a476e8d0dedbbf9be43c0",
            "d480ed5f89ff49a689b94fedc379e805",
            "667c77b001314a609d8a632f27f5e5eb",
            "09cf8a1ae7934685b947a3946189ab05",
            "398e959a79354da38489618ff07d4609",
            "563f4c6072b5419c86db717cbd596236",
            "e9a8fe4749db462b9fc584073e09d524",
            "2b7f80160f114f84bfc145f6f770137e",
            "f2cf406ae63d42769f4b88ca97bd7ccc",
            "817705a39e8e4e99b6133276eb62f5ae",
            "6eeae9eb0e414d009abc1e4ca6172a8b"
          ]
        },
        "id": "pf7UrrmXCBRa",
        "outputId": "ec3c39e0-a664-41e5-9a9c-0d7913ce2fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Baseline Model (FP16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1e644cf761a476e8d0dedbbf9be43c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Benchmarking Model: LLaMA-2-7B (FP16) ---\n",
            "VRAM Memory Footprint: 12.55 GB\n",
            "Warming up GPU\n",
            "Generating text for measurement\n",
            "Time: 6.03s | Speed: 16.58 tokens/s\n",
            "------------------------------\n",
            "Cleaning up FP16 model to free VRAM\n",
            "\n",
            " GPU Memory flushed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUANTIZED MODEL BENCHMARK (8-bit)"
      ],
      "metadata": {
        "id": "SkCamW2Cvqin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Quantized Model (8-bit)\")\n",
        "\n",
        "# 1. Define Quantization Configuration\n",
        "# This is where we apply the theory from the \"8-bit LLM\" paper.\n",
        "# - load_in_8bit=True: Activates the bitsandbytes library to compress weights.\n",
        "# - llm_int8_threshold=6.0: This is the \"Mixed Precision\" trick.\n",
        "# Any value (outlier) larger than 6.0 is kept in FP16 to preserve accuracy.\n",
        "# This threshold comes from the LLM.int(8) paper\n",
        "# Everything else is compressed to 8-bit.\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_threshold=6.0\n",
        ")\n",
        "\n",
        "# 2. Load the Model with Quantization\n",
        "# We pass the 'quantization_config' argument. The model is compressed on-the-fly\n",
        "# while being loaded into the GPU.\n",
        "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "# 3. Execute the Benchmark\n",
        "# We use the same function and prompt to ensure a fair comparison.\n",
        "res_8bit = run_benchmark(model_8bit, tokenizer, \"LLaMA-2-7B (8-bit)\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "6e55a612604340b2981526cd66536b6a",
            "12aeb463a225488da1d6f81949f15c3f",
            "6beccb3dceb146178fcea7c8910274e9",
            "819c5b74ecb74e1d83b5e753c16550cd",
            "286710677627418195dd976618c61d51",
            "9cc7221f23a04da2bd7f7210411a6e96",
            "c0d1364b6d114dafae3ba3c08eab3c8f",
            "cad1737498174907b213bf0e6f013e7d",
            "2e7d845c950f4143b0ac5fef3bf4d885",
            "933b91dc2e5c4336be0b9047e2545338",
            "65fc39dca8bb4deaafaea9b2200d0f72"
          ]
        },
        "id": "96PpwKz0FX9x",
        "outputId": "a56e921f-a0c9-41e6-edf8-be9211b457f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Quantized Model (8-bit)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e55a612604340b2981526cd66536b6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Benchmarking Model: LLaMA-2-7B (8-bit) ---\n",
            "VRAM Memory Footprint: 6.53 GB\n",
            "Warming up GPU\n",
            "Generating text for measurement\n",
            "Time: 14.76s | Speed: 6.77 tokens/s\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINAL RESULTS & COMPARISON"
      ],
      "metadata": {
        "id": "mxmiCFnhxuNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FINAL COMPARATIVE RESULTS\")\n",
        "\n",
        "# We create a dataFrame to visualize the differences\n",
        "df = pd.DataFrame([res_fp16, res_8bit])\n",
        "\n",
        "print(df[[\"Model\", \"Memory (GB)\", \"Latency (s)\", \"Tokens/Sec\"]])\n",
        "\n",
        "# A quick quality check printing only the 200 first generated characters\n",
        "print(\"\\n--- Quick Quality Check: FP16 Output ---\")\n",
        "print(res_fp16[\"Output Sample quick\"])\n",
        "print(\"\\n--- Quick Quality Check: 8-bit Output ---\")\n",
        "print(res_8bit[\"Output Sample quick\"])\n",
        "\n",
        "# Full comparison with the entire generated answer\n",
        "print(\"\\n--- Quality Check: FP16 Output ---\")\n",
        "print(res_fp16[\"Output Sample\"])\n",
        "print(\"\\n--- Quality Check: 8-bit Output ---\")\n",
        "print(res_8bit[\"Output Sample\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "233k5207FZYx",
        "outputId": "844223b0-f927-45a7-f8ed-d43b0762bbaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL COMPARATIVE RESULTS\n",
            "                Model  Memory (GB)  Latency (s)  Tokens/Sec\n",
            "0   LLaMA-2-7B (FP16)        12.55         6.03       16.58\n",
            "1  LLaMA-2-7B (8-bit)         6.53        14.76        6.77\n",
            "\n",
            "--- Quick Quality Check: FP16 Output ---\n",
            "Write a detailed plan to visit Paris in 3 days, focusing on museums and food. Here are some of the top museums and food experiences you could include:\n",
            "\n",
            "Day 1:\n",
            "\n",
            "* Start the day at the Louvre Museum, sp...\n",
            "\n",
            "--- Quick Quality Check: 8-bit Output ---\n",
            "Write a detailed plan to visit Paris in 3 days, focusing on museums and food.\n",
            "Day 1: Museums\n",
            "Stop 1: The Louvre Museum (9:00 am - 12:00 pm)\n",
            "* Start your day at the world-famous Louvre Museum, home to ...\n",
            "\n",
            "--- Quality Check: FP16 Output ---\n",
            "Write a detailed plan to visit Paris in 3 days, focusing on museums and food. Here are some of the top museums and food experiences you could include:\n",
            "\n",
            "Day 1:\n",
            "\n",
            "* Start the day at the Louvre Museum, spending at least 2-3 hours exploring the collection, including the Mona Lisa.\n",
            "* After lunch, head to the Musée d'Orsay to see the Impressionist and Post-Impressionist art collection, including works by Monet, Renoir, and Van Gogh.\n",
            "*\n",
            "\n",
            "--- Quality Check: 8-bit Output ---\n",
            "Write a detailed plan to visit Paris in 3 days, focusing on museums and food.\n",
            "Day 1: Museums\n",
            "Stop 1: The Louvre Museum (9:00 am - 12:00 pm)\n",
            "* Start your day at the world-famous Louvre Museum, home to the Mona Lisa and countless other works of art.\n",
            "* Spend 3-4 hours exploring the museum's vast collection, which includes Egyptian antiquities, Greek and Roman sculpture, and Renaissance paintings.\n",
            "*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To conclude, we do not see any difference between the two answers in terms of quality (the quantized version is not telling us to go to Trafalgar Square while we asked for recommendations for Paris)\n",
        "\n",
        "Moreover, the quantized version is performing as it is meant to, in the sense that it uses less memory than the 16-bit version."
      ],
      "metadata": {
        "id": "mHq83DY4CvGo"
      }
    }
  ]
}